{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNthw0RE+EB11rodWLHjKc+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GustaFTW/TensorFlow-ZTM/blob/main/section9_and_milestone2_skimlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Milestone Project 2: SkimLit\n",
        "\n",
        "The purpose of this notebook is to build an NLP model to make reading medical abstracts easier.\n",
        "\n",
        "The paper we're replicating (the source of the dataset that we'll be using) is available here: https://arxiv.org/abs/1710.06071\n",
        "\n",
        "And reading through the paper above, we see that the model architecture that they use is available here: https://arxiv.org/abs/1612.05251\n",
        "\n",
        "**Resource:** If you want to find the ground truth for this notebook (with lots of diagrams and text annotations) see the GitHub: https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/09_SkimLit_nlp_milestone_project_2.ipynb"
      ],
      "metadata": {
        "id": "FvTwSY1giFUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confirm access to a GPU\n"
      ],
      "metadata": {
        "id": "4ioIrc4cuT8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cia5z2XvJ4T",
        "outputId": "81d96e49-bf24-4eef-e829-128ed36fa8bc"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 13 14:36:46 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0    32W /  70W |   3290MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      2025      C                                    3287MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get data\n",
        "\n",
        "Since we'll be replication the paper above (PubMed 200k RCT), let's download the dataset they used.\n",
        "\n",
        "We can do so from the authors GitHub: https://github.com/Franck-Dernoncourt/pubmed-rct"
      ],
      "metadata": {
        "id": "_SpC__q6vL0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct\n",
        "!ls pubmed-rct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRY8WD7Hvm1G",
        "outputId": "1b85b831-c096-43de-ee60-aa0a1916f0e9"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'pubmed-rct' already exists and is not an empty directory.\n",
            "PubMed_200k_RCT\n",
            "PubMed_200k_RCT_numbers_replaced_with_at_sign\n",
            "PubMed_20k_RCT\n",
            "PubMed_20k_RCT_numbers_replaced_with_at_sign\n",
            "README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what files are in the PubMed_20k dataset\n",
        "!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFiIO3PTvufq",
        "outputId": "e6c71f8d-2a6b-449a-ca53-a7b257066285"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev.txt  test.txt  train.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start our experiments using the 20k dataset with numbers replaced by \"@\" sign\n",
        "data_dir = \"/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign\""
      ],
      "metadata": {
        "id": "m0BIiQIwwOvt"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check all of the filenames in the target directory\n",
        "import os\n",
        "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
        "filenames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpp0JeIcxsID",
        "outputId": "4b9d7745-b3af-4623-c748-fe1bea5a253b"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_signtrain.txt',\n",
              " '/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_signdev.txt',\n",
              " '/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_signtest.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess data\n",
        "\n",
        "Now we've got some text data, it's time to become one with it.\n",
        "\n",
        "And one of the best ways to become one with the data is to...\n",
        "\n",
        "> Visualize, visualize, visualize\n",
        "\n",
        "So with that in mind, let's write a function to read in all of the lines of a target text file."
      ],
      "metadata": {
        "id": "aEU6Sp7qxzl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function to read the lines of a document\n",
        "def get_lines(filename): \n",
        "    \"\"\"\n",
        "    Reads filename (a text filename) and returns the lines of text as a list.\n",
        "    \n",
        "    Args:\n",
        "      filename: a string containing the targe filepath.\n",
        "    \n",
        "\n",
        "    Returns:\n",
        "      A list of strings with one string per line from the target filename.\n",
        "    \"\"\"\n",
        "\n",
        "    with open(filename, \"r\") as f:\n",
        "        return f.readlines()"
      ],
      "metadata": {
        "id": "osr5qTlcycK8"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's read in the training lines\n",
        "train_lines = get_lines(data_dir+\"/train.txt\") # read the lines with the training file"
      ],
      "metadata": {
        "id": "FAmKA-dOzDnf"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's think about how er want our data to look...\n",
        "\n",
        "How I think our data would be best represented...\n",
        "```\n",
        "    [{line_number\": 0,\n",
        "   \"target\": \"BACKGROUND\",\n",
        "   \"text\": \"some string.\\n\",\n",
        "   \"total_lines: 11},\n",
        "   ...]\n",
        "```"
      ],
      "metadata": {
        "id": "QmPqegBmzJ-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text_with_line_numbers(filename):\n",
        "    \"\"\"\n",
        "    Returns a list of a dictionaries of absctract line data.\n",
        "\n",
        "    Takes in filename, reads it contents and sorts through each line,\n",
        "    extracting things like the target label, the text of the sentence, \n",
        "    how many sentences are in the current abstract and what sentence\n",
        "    number the target line is.\n",
        "    \"\"\"\n",
        "    input_lines = get_lines(filename) # get all lines from filename\n",
        "    abstract_lines = \"\" # create an empty abstract\n",
        "    abstract_samples = [] # create an empty list of absctracts\n",
        "\n",
        "    # Loop through each line in the target file\n",
        "    for line in input_lines:\n",
        "      if line.startswith(\"###\"): #check to see if the line is an ID line\n",
        "          abstract_id = line\n",
        "          abstract_lines = \"\" # reset the abstract string if the line is an ID line\n",
        "      \n",
        "      elif line.isspace(): # check to see if line is a new line\n",
        "          abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines\n",
        "          \n",
        "          # Iterate through each line in  a single abstract and count them at the same time\n",
        "          for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
        "              line_data = {} # create an empty dictionary for each line\n",
        "              target_text_split = abstract_line.split(\"\\t\") # split target label from text\n",
        "              line_data[\"target\"] = target_text_split[0] # get target label\n",
        "              line_data[\"text\"] = target_text_split[1].lower() # get target text and lower \n",
        "              line_data[\"line_number\"] = abstract_line_number # what number line does the line apper in\n",
        "              line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are there in the target abstract? (start from 0)\n",
        "              abstract_samples.append(line_data) # add line data to abstract samples list\n",
        "\n",
        "      else: # if the above conditions aren't fulffiled, the line contains a labelled sentence\n",
        "          abstract_lines += line\n",
        "    return abstract_samples\n"
      ],
      "metadata": {
        "id": "0Y05Um061H8Z"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data from file and preprocess it\n",
        "%%time\n",
        "train_samples = preprocess_text_with_line_numbers(data_dir + \"/train.txt\")\n",
        "val_samples = preprocess_text_with_line_numbers(data_dir + \"/dev.txt\")\n",
        "test_samples = preprocess_text_with_line_numbers(data_dir + \"/test.txt\")\n",
        "print(len(train_samples), len(val_samples), len(test_samples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_CAK03a6E3U",
        "outputId": "294260ea-c981-4a25-a612-60010e5dd528"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "180040 30212 30135\n",
            "CPU times: user 493 ms, sys: 84 ms, total: 577 ms\n",
            "Wall time: 577 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first abstract of our training tada\n",
        "train_samples[:14]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NOiTGqE7bMI",
        "outputId": "084fd921-4eb7-4012-a7ec-2c99dde3420f"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'target': 'OBJECTIVE',\n",
              "  'text': 'to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
              "  'line_number': 0,\n",
              "  'total_lines': 11},\n",
              " {'target': 'METHODS',\n",
              "  'text': 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
              "  'line_number': 1,\n",
              "  'total_lines': 11},\n",
              " {'target': 'METHODS',\n",
              "  'text': 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
              "  'line_number': 2,\n",
              "  'total_lines': 11},\n",
              " {'target': 'METHODS',\n",
              "  'text': 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
              "  'line_number': 3,\n",
              "  'total_lines': 11},\n",
              " {'target': 'METHODS',\n",
              "  'text': 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n",
              "  'line_number': 4,\n",
              "  'total_lines': 11},\n",
              " {'target': 'METHODS',\n",
              "  'text': 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n",
              "  'line_number': 5,\n",
              "  'total_lines': 11},\n",
              " {'target': 'RESULTS',\n",
              "  'text': 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n",
              "  'line_number': 6,\n",
              "  'total_lines': 11},\n",
              " {'target': 'RESULTS',\n",
              "  'text': 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n",
              "  'line_number': 7,\n",
              "  'total_lines': 11},\n",
              " {'target': 'RESULTS',\n",
              "  'text': 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n",
              "  'line_number': 8,\n",
              "  'total_lines': 11},\n",
              " {'target': 'RESULTS',\n",
              "  'text': 'these differences remained significant at @ weeks .',\n",
              "  'line_number': 9,\n",
              "  'total_lines': 11},\n",
              " {'target': 'RESULTS',\n",
              "  'text': 'the outcome measures in rheumatology clinical trials-osteoarthritis research society international responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .',\n",
              "  'line_number': 10,\n",
              "  'total_lines': 11},\n",
              " {'target': 'CONCLUSIONS',\n",
              "  'text': 'low-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee oa ( clinicaltrials.gov identifier nct@ ) .',\n",
              "  'line_number': 11,\n",
              "  'total_lines': 11},\n",
              " {'target': 'BACKGROUND',\n",
              "  'text': 'emotional eating is associated with overeating and the development of obesity .',\n",
              "  'line_number': 0,\n",
              "  'total_lines': 10},\n",
              " {'target': 'BACKGROUND',\n",
              "  'text': 'yet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .',\n",
              "  'line_number': 1,\n",
              "  'total_lines': 10}]"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our data is in the format of a list of dictionaries, how about we turn it into a DataFrame to further visualize it?"
      ],
      "metadata": {
        "id": "dqLqG2nk7jqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.DataFrame(train_samples)\n",
        "val_df = pd.DataFrame(val_samples)\n",
        "test_df = pd.DataFrame(test_samples)\n",
        "train_df.head(14)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "41J1trAQCOdy",
        "outputId": "ac9209ae-6d93-4d0b-da13-1f7efc3da434"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         target                                               text  \\\n",
              "0     OBJECTIVE  to investigate the efficacy of @ weeks of dail...   \n",
              "1       METHODS  a total of @ patients with primary knee oa wer...   \n",
              "2       METHODS  outcome measures included pain reduction and i...   \n",
              "3       METHODS  pain was assessed using the visual analog pain...   \n",
              "4       METHODS  secondary outcome measures included the wester...   \n",
              "5       METHODS  serum levels of interleukin @ ( il-@ ) , il-@ ...   \n",
              "6       RESULTS  there was a clinically relevant reduction in t...   \n",
              "7       RESULTS  the mean difference between treatment arms ( @...   \n",
              "8       RESULTS  further , there was a clinically relevant redu...   \n",
              "9       RESULTS  these differences remained significant at @ we...   \n",
              "10      RESULTS  the outcome measures in rheumatology clinical ...   \n",
              "11  CONCLUSIONS  low-dose oral prednisolone had both a short-te...   \n",
              "12   BACKGROUND  emotional eating is associated with overeating...   \n",
              "13   BACKGROUND  yet , empirical evidence for individual ( trai...   \n",
              "\n",
              "    line_number  total_lines  \n",
              "0             0           11  \n",
              "1             1           11  \n",
              "2             2           11  \n",
              "3             3           11  \n",
              "4             4           11  \n",
              "5             5           11  \n",
              "6             6           11  \n",
              "7             7           11  \n",
              "8             8           11  \n",
              "9             9           11  \n",
              "10           10           11  \n",
              "11           11           11  \n",
              "12            0           10  \n",
              "13            1           10  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-17db0697-d3ec-47bf-bbea-1a987d3e177a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "      <th>line_number</th>\n",
              "      <th>total_lines</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OBJECTIVE</td>\n",
              "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>a total of @ patients with primary knee oa wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>outcome measures included pain reduction and i...</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>pain was assessed using the visual analog pain...</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>secondary outcome measures included the wester...</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>serum levels of interleukin @ ( il-@ ) , il-@ ...</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>there was a clinically relevant reduction in t...</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>the mean difference between treatment arms ( @...</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>further , there was a clinically relevant redu...</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>these differences remained significant at @ we...</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>RESULTS</td>\n",
              "      <td>the outcome measures in rheumatology clinical ...</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>CONCLUSIONS</td>\n",
              "      <td>low-dose oral prednisolone had both a short-te...</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>BACKGROUND</td>\n",
              "      <td>emotional eating is associated with overeating...</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>BACKGROUND</td>\n",
              "      <td>yet , empirical evidence for individual ( trai...</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17db0697-d3ec-47bf-bbea-1a987d3e177a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-17db0697-d3ec-47bf-bbea-1a987d3e177a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-17db0697-d3ec-47bf-bbea-1a987d3e177a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of labels in training data\n",
        "train_df.target.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Yp4U0GMCYIu",
        "outputId": "a6796dc1-56cf-4f78-d42d-0d29e87a796b"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "METHODS        59353\n",
              "RESULTS        57953\n",
              "CONCLUSIONS    27168\n",
              "BACKGROUND     21727\n",
              "OBJECTIVE      13839\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the length of different lines\n",
        "train_df.total_lines.plot.hist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "hA8tOcRaCwEq",
        "outputId": "0d7026b2-2b8e-4a9f-cdd3-575590f2d1ec"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fbb81ab3df0>"
            ]
          },
          "metadata": {},
          "execution_count": 182
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAD6CAYAAABgZXp6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXpUlEQVR4nO3df7BfdX3n8efLRCpSkVDSLJNgg21Gl7r+gCvg1HatjCHg1tBdl4WtS5ZhiDNgV8f9QXQ6i8Uyk+5spdJatqlkTVwV8SfZEppGxHb7Bz+CIAjo5IqwJAJJDRDRFhZ97x/fz5Wv4ebyzbn53i/35vmY+c49530+55zPZ74TXpxzPt/vN1WFJEldvGjUHZAkzV6GiCSpM0NEktSZISJJ6swQkSR1ZohIkjobWogkeVWSO/tee5O8L8nRSbYm2d7+Lmjtk+TKJONJ7kpyYt+xVrX225Os6quflOTuts+VSTKs8UiSnisz8TmRJPOAncApwMXAnqpam2QNsKCqLklyJvC7wJmt3Uer6pQkRwPbgDGggNuBk6rqsSS3Av8BuAXYDFxZVTdM1Zdjjjmmli5dOpRxStJcdPvtt/99VS2cbNv8GerDacB3qurBJCuBt7T6BuBrwCXASmBj9VLt5iRHJTm2td1aVXsAkmwFViT5GnBkVd3c6huBs4ApQ2Tp0qVs27bt4I5OkuawJA/ub9tMPRM5B/hMW15UVQ+35UeARW15MfBQ3z47Wm2q+o5J6pKkGTL0EElyGPAO4HP7bmtXHUO/n5ZkdZJtSbbt3r172KeTpEPGTFyJnAF8vaoebeuPtttUtL+7Wn0ncFzffktabar6kknqz1FV66pqrKrGFi6c9LaeJKmDmQiRc3n2VhbAJmBihtUq4Lq++nltltapwBPtttcWYHmSBW0m13JgS9u2N8mpbVbWeX3HkiTNgKE+WE9yBPA24N195bXAtUkuAB4Ezm71zfRmZo0DPwLOB6iqPUk+DNzW2l028ZAduAj4BHA4vQfqUz5UlyQdXDMyxfeFZGxsrJydJUmDS3J7VY1Nts1PrEuSOjNEJEmdGSKSpM5m6hPrmqWWrrl+JOd9YO3bR3JeSQfGKxFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSps6GGSJKjknw+ybeS3JfkTUmOTrI1yfb2d0FrmyRXJhlPcleSE/uOs6q1355kVV/9pCR3t32uTJJhjkeS9LOGfSXyUeCvqurVwOuA+4A1wI1VtQy4sa0DnAEsa6/VwFUASY4GLgVOAU4GLp0Intbmwr79Vgx5PJKkPkMLkSQvB34DuBqgqp6uqseBlcCG1mwDcFZbXglsrJ6bgaOSHAucDmytqj1V9RiwFVjRth1ZVTdXVQEb+44lSZoBw7wSOR7YDfzPJHck+XiSI4BFVfVwa/MIsKgtLwYe6tt/R6tNVd8xSV2SNEOGGSLzgROBq6rqDcAPefbWFQDtCqKG2AcAkqxOsi3Jtt27dw/7dJJ0yBhmiOwAdlTVLW398/RC5dF2K4r2d1fbvhM4rm//Ja02VX3JJPXnqKp1VTVWVWMLFy6c1qAkSc8aWohU1SPAQ0le1UqnAfcCm4CJGVargOva8ibgvDZL61TgiXbbawuwPMmC9kB9ObClbdub5NQ2K+u8vmNJkmbA/CEf/3eBTyU5DLgfOJ9ecF2b5ALgQeDs1nYzcCYwDvyotaWq9iT5MHBba3dZVe1pyxcBnwAOB25oL0nSDBlqiFTVncDYJJtOm6RtARfv5zjrgfWT1LcBr5lmNyVJHfmJdUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOhtqiCR5IMndSe5Msq3Vjk6yNcn29ndBqyfJlUnGk9yV5MS+46xq7bcnWdVXP6kdf7ztm2GOR5L0s2biSuQ3q+r1VTXW1tcAN1bVMuDGtg5wBrCsvVYDV0EvdIBLgVOAk4FLJ4Kntbmwb78Vwx+OJGnCKG5nrQQ2tOUNwFl99Y3VczNwVJJjgdOBrVW1p6oeA7YCK9q2I6vq5qoqYGPfsSRJM2DYIVLAXye5PcnqVltUVQ+35UeARW15MfBQ3747Wm2q+o5J6s+RZHWSbUm27d69ezrjkST1mT/k47+5qnYm+UVga5Jv9W+sqkpSQ+4DVbUOWAcwNjY29PNJ0qFiqFciVbWz/d0FfIneM41H260o2t9drflO4Li+3Ze02lT1JZPUJUkzZGghkuSIJC+bWAaWA98ENgETM6xWAde15U3AeW2W1qnAE+221xZgeZIF7YH6cmBL27Y3yaltVtZ5fceSJM2AYd7OWgR8qc26nQ98uqr+KsltwLVJLgAeBM5u7TcDZwLjwI+A8wGqak+SDwO3tXaXVdWetnwR8AngcOCG9pIkzZChhUhV3Q+8bpL694HTJqkXcPF+jrUeWD9JfRvwmml3VpLUiZ9YlyR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktTZQCGS5J8NuyOSpNln0CuRP0tya5KLkrx8qD2SJM0aA4VIVf068DvAccDtST6d5G1D7Zkk6QVv4GciVbUd+D3gEuCfA1cm+VaSfzmszkmSXtgGfSby2iRXAPcBbwV+q6r+aVu+Yoj9kyS9gM0fsN2fAB8HPlhV/zBRrKrvJfm9ofRMkvSCN+jtrLcDn54IkCQvSvJSgKr65FQ7JpmX5I4kf9nWj09yS5LxJJ9Nclir/1xbH2/bl/Yd4wOt/u0kp/fVV7TaeJI1BzJwSdL0DRoiXwEO71t/aasN4r30boNN+EPgiqr6FeAx4IJWvwB4rNWvaO1IcgJwDvCrwAp6M8XmJZkHfAw4AzgBOLe1lSTNkEFvZ72kqp6cWKmqJyeuRKaSZAm9q5jLgfcnCb3nKP+2NdkAfAi4CljZlgE+D/xpa78SuKaqngK+m2QcOLm1G6+q+9u5rmlt7x1wTHoBW7rm+pGd+4G1bx/ZuaXZZtArkR8mOXFiJclJwD9M0X7CHwP/BfhJW/8F4PGqeqat7wAWt+XFwEMAbfsTrf1P6/vss7+6JGmGDHol8j7gc0m+BwT4J8C/mWqHJP8C2FVVtyd5y7R6OU1JVgOrAV7xileMsiuSNKcMFCJVdVuSVwOvaqVvV9X/e57dfg14R5IzgZcARwIfBY5KMr9dbSwBdrb2O+l9mHFHkvnAy4Hv99Un9O+zv/q+/V8HrAMYGxur5+m3JGlAB/IFjG8EXgucSO8h9nlTNa6qD1TVkqpaSu/B+Fer6neAm4B3tmargOva8qa2Ttv+1aqqVj+nzd46HlgG3ArcBixrs70Oa+fYdADjkSRN00BXIkk+CfwycCfw41YuYGOHc14CXJPkD4A7gKtb/Wrgk+3B+R56oUBV3ZPkWnoPzJ8BLq6qH7d+vQfYAswD1lfVPR36I0nqaNBnImPACe3K4IBV1deAr7Xl+3l2dlV/m38E/vV+9r+c3gyvfeubgc1d+iRJmr5Bb2d9k97DdEmSfmrQK5FjgHuT3Ao8NVGsqncMpVeSpFlh0BD50DA7IUmanQad4vs3SX4JWFZVX2mfVp833K5Jkl7oBv0q+AvpfRXJn7fSYuDLw+qUJGl2GPTB+sX0Pjy4F376A1W/OKxOSZJmh0FD5KmqenpipX2i3E9+S9IhbtAQ+ZskHwQOb7+t/jngfw+vW5Kk2WDQEFkD7AbuBt5N7wN+/qKhJB3iBp2d9RPgL9pLkiRg8O/O+i6TPAOpqlce9B5JkmaNA/nurAkvofcdV0cf/O5IkmaTgZ6JVNX3+147q+qP6f3srSTpEDbo7awT+1ZfRO/KZNCrGEnSHDVoEPxR3/IzwAPA2Qe9N5KkWWXQ2Vm/OeyOSJJmn0FvZ71/qu1V9ZGD0x1J0mxyILOz3sizv2H+W/R+53z7MDoljdLSNdeP5LwPrHWuimafQUNkCXBiVf0AIMmHgOur6l3D6pgk6YVv0K89WQQ83bf+dKtJkg5hg16JbARuTfKltn4WsGE4XZIkzRaDzs66PMkNwK+30vlVdcfwuiVJmg0GvZ0F8FJgb1V9FNiR5PipGid5SZJbk3wjyT1Jfr/Vj09yS5LxJJ9Nclir/1xbH2/bl/Yd6wOt/u0kp/fVV7TaeJI1BzAWSdJBMOjP414KXAJ8oJVeDPyv59ntKeCtVfU64PXAiiSnAn8IXFFVvwI8BlzQ2l8APNbqV7R2JDkBOAf4VWAF8GdJ5iWZB3wMOAM4ATi3tZUkzZBBr0R+G3gH8EOAqvoe8LKpdqieJ9vqi9urgLfS+7126D1XOastr+TZ5yyfB05Lkla/pqqeqqrvAuPAye01XlX3t19dvKa1lSTNkEFD5OmqKtrXwSc5YpCd2hXDncAuYCvwHeDxqnqmNdkBLG7Li4GHANr2J4Bf6K/vs8/+6pKkGTJoiFyb5M+Bo5JcCHyFAX6gqqp+XFWvp/c5k5OBV3fu6TQkWZ1kW5Jtu3fvHkUXJGlOet7ZWe2W0mfpBcBe4FXAf62qrYOepKoeT3IT8CZ6QTS/XW0sAXa2ZjuB4+g9tJ8PvBz4fl99Qv8++6vve/51wDqAsbGx5/y4liSpm+e9Emm3sTZX1daq+s9V9Z8GCZAkC5Mc1ZYPB94G3AfcBLyzNVsFXNeWN7V12vavtnNvAs5ps7eOB5bR+8qV24BlbbbXYfQevk98LYskaQYM+mHDryd5Y1XddgDHPhbY0GZRvQi4tqr+Msm9wDVJ/gC4A7i6tb8a+GSScWAPvVCgqu5Jci1wL72vob+4qn4MkOQ9wBZgHrC+qu45gP5JkqZp0BA5BXhXkgfozdAKvYuU1+5vh6q6C3jDJPX76T0f2bf+j/R+dneyY10OXD5JfTOwebAhSJIOtilDJMkrqur/AqdP1U6SdGh6viuRL9P79t4Hk3yhqv7VTHRKkjQ7PN+D9fQtv3KYHZEkzT7PFyK1n2VJkp73dtbrkuyld0VyeFuGZx+sHznU3kmSXtCmDJGqmjdTHZEkzT4H8lXwkiT9DENEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktTZoD9KpRFauub6UXdBkibllYgkqTNDRJLUmSEiSerMEJEkdWaISJI6G1qIJDkuyU1J7k1yT5L3tvrRSbYm2d7+Lmj1JLkyyXiSu5Kc2HesVa399iSr+uonJbm77XNlkjy3J5KkYRnmlcgzwH+sqhOAU4GLk5wArAFurKplwI1tHeAMYFl7rQaugl7oAJcCpwAnA5dOBE9rc2HffiuGOB5J0j6GFiJV9XBVfb0t/wC4D1gMrAQ2tGYbgLPa8kpgY/XcDByV5FjgdGBrVe2pqseArcCKtu3Iqrq5qgrY2HcsSdIMmJFnIkmWAm8AbgEWVdXDbdMjwKK2vBh4qG+3Ha02VX3HJPXJzr86ybYk23bv3j2tsUiSnjX0EEny88AXgPdV1d7+be0Koobdh6paV1VjVTW2cOHCYZ9Okg4ZQw2RJC+mFyCfqqovtvKj7VYU7e+uVt8JHNe3+5JWm6q+ZJK6JGmGDHN2VoCrgfuq6iN9mzYBEzOsVgHX9dXPa7O0TgWeaLe9tgDLkyxoD9SXA1vatr1JTm3nOq/vWJKkGTDML2D8NeDfAXcnubPVPgisBa5NcgHwIHB227YZOBMYB34EnA9QVXuSfBi4rbW7rKr2tOWLgE8AhwM3tJckaYYMLUSq6u+A/X1u47RJ2hdw8X6OtR5YP0l9G/CaaXRTkjQNfmJdktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnQ0tRJKsT7IryTf7akcn2Zpke/u7oNWT5Mok40nuSnJi3z6rWvvtSVb11U9Kcnfb58okGdZYJEmTmz/EY38C+FNgY19tDXBjVa1NsqatXwKcASxrr1OAq4BTkhwNXAqMAQXcnmRTVT3W2lwI3AJsBlYANwxxPNJQLV1z/UjO+8Dat4/kvJobhnYlUlV/C+zZp7wS2NCWNwBn9dU3Vs/NwFFJjgVOB7ZW1Z4WHFuBFW3bkVV1c1UVvaA6C0nSjJrpZyKLqurhtvwIsKgtLwYe6mu3o9Wmqu+YpC5JmkEje7DeriBqJs6VZHWSbUm27d69eyZOKUmHhJkOkUfbrSja312tvhM4rq/dklabqr5kkvqkqmpdVY1V1djChQunPQhJUs9Mh8gmYGKG1Srgur76eW2W1qnAE+221xZgeZIFbSbXcmBL27Y3yaltVtZ5fceSJM2Qoc3OSvIZ4C3AMUl20JtltRa4NskFwIPA2a35ZuBMYBz4EXA+QFXtSfJh4LbW7rKqmnhYfxG9GWCH05uV5cwsSZphQwuRqjp3P5tOm6RtARfv5zjrgfWT1LcBr5lOHyVJ0+Mn1iVJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSps/mj7oCk0Vq65vqRnfuBtW8f2bl1cHglIknqbNZfiSRZAXwUmAd8vKrWDutco/w/NmkuGtW/Ka+ADp5ZfSWSZB7wMeAM4ATg3CQnjLZXknTomNUhApwMjFfV/VX1NHANsHLEfZKkQ8Zsv521GHiob30HcMqI+iJplnAywcEz20NkIElWA6vb6pNJvj3K/kziGODvR92JIZvrY3R8s9+MjDF/OOwz7Nd0xvdL+9sw20NkJ3Bc3/qSVvsZVbUOWDdTnTpQSbZV1dio+zFMc32Mjm/2m+tjHNb4ZvszkduAZUmOT3IYcA6wacR9kqRDxqy+EqmqZ5K8B9hCb4rv+qq6Z8TdkqRDxqwOEYCq2gxsHnU/pukFe6vtIJrrY3R8s99cH+NQxpeqGsZxJUmHgNn+TESSNEKGyIgleSDJ3UnuTLJt1P05GJKsT7IryTf7akcn2Zpke/u7YJR9nI79jO9DSXa29/HOJGeOso/TkeS4JDcluTfJPUne2+pz4j2cYnxz6T18SZJbk3yjjfH3W/34JLckGU/y2TYhaXrn8nbWaCV5ABirqjkzBz/JbwBPAhur6jWt9t+APVW1NskaYEFVXTLKfna1n/F9CHiyqv77KPt2MCQ5Fji2qr6e5GXA7cBZwL9nDryHU4zvbObOexjgiKp6MsmLgb8D3gu8H/hiVV2T5H8A36iqq6ZzLq9EdNBV1d8Ce/YprwQ2tOUN9P7Rzkr7Gd+cUVUPV9XX2/IPgPvofTvEnHgPpxjfnFE9T7bVF7dXAW8FPt/qB+U9NERGr4C/TnJ7+2T9XLWoqh5uy48Ai0bZmSF5T5K72u2uWXmrZ19JlgJvAG5hDr6H+4wP5tB7mGRekjuBXcBW4DvA41X1TGuyg4MQnobI6L25qk6k903EF7dbJXNa9e6hzrX7qFcBvwy8HngY+KPRdmf6kvw88AXgfVW1t3/bXHgPJxnfnHoPq+rHVfV6et/kcTLw6mGcxxAZsara2f7uAr5E782eix5t96In7knvGnF/DqqqerT9o/0J8BfM8vex3Uf/AvCpqvpiK8+Z93Cy8c2193BCVT0O3AS8CTgqycTnAyf9mqgDZYiMUJIj2oM9khwBLAe+OfVes9YmYFVbXgVcN8K+HHQT/3FtfptZ/D62h7JXA/dV1Uf6Ns2J93B/45tj7+HCJEe15cOBt9F79nMT8M7W7KC8h87OGqEkr6R39QG9bw/4dFVdPsIuHRRJPgO8hd63hj4KXAp8GbgWeAXwIHB2Vc3Kh9P7Gd9b6N0GKeAB4N19zw9mlSRvBv4PcDfwk1b+IL3nBrP+PZxifOcyd97D19J7cD6P3sXCtVV1WftvzjXA0cAdwLuq6qlpncsQkSR15e0sSVJnhogkqTNDRJLUmSEiSerMEJEkdWaISJI6M0QkSZ0ZIpKkzv4/2LyLCkd/AwYAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get list of sentences"
      ],
      "metadata": {
        "id": "9MTL4DczDC71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert abstract text lines into lists\n",
        "train_sentences = train_df[\"text\"].tolist()\n",
        "val_sentences = val_df[\"text\"].tolist()\n",
        "test_sentences = test_df[\"text\"].tolist()"
      ],
      "metadata": {
        "id": "rOaIu17LDOo5"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first 10 lines of training sentences\n",
        "train_sentences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR9ipYfeDYgT",
        "outputId": "634769fa-56b4-44ec-eed8-c2d129393c04"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
              " 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
              " 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
              " 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
              " 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n",
              " 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n",
              " 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n",
              " 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n",
              " 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n",
              " 'these differences remained significant at @ weeks .']"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make numeric labels (ML models require numeric labels)"
      ],
      "metadata": {
        "id": "5dpzx9L6DiID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encode labels\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "one_hot_encoder = OneHotEncoder(sparse=False) # we want a non-sparse matrix\n",
        "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "val_labels_one_hot = one_hot_encoder.fit_transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "test_labels_one_hot = one_hot_encoder.fit_transform(test_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "# check what one hot encoded labels looks like\n",
        "train_labels_one_hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDUIR29uEVyr",
        "outputId": "9c802ae6-7b64-40fe-afec-97b56cca6c33"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label encode labels"
      ],
      "metadata": {
        "id": "5oQfVZZLE2kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract labels (\"target\" columns) and encode them into integers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
        "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
        "test_label_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())\n",
        "\n",
        "# Check what training labels look like\n",
        "train_labels_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQFQI7whGFuh",
        "outputId": "b479ea26-1ae9-4d90-8209-71633e1c94ee"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 2, 2, ..., 4, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get class names and number of classes from LabelEncoder instance\n",
        "num_classes = len(label_encoder.classes_)\n",
        "class_names = label_encoder.classes_\n",
        "num_classes, class_names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-EvRfzTGhgn",
        "outputId": "60894e3e-4860-41ae-db60-7ea3215fc0d2"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
              "       dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Starting a series of modelling experiments...\n",
        "\n",
        "As usual, we're going to be trying out a bunch of different models and seeing which one works best.\n",
        "\n",
        "And as always, we're going to start with a baseline (TF-IDF Multinomial Naive Bayes classifier)."
      ],
      "metadata": {
        "id": "kx12Yltupu7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model: 0 Getting a baseline\n"
      ],
      "metadata": {
        "id": "NbTaQXkAG6Vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create a pipeline\n",
        "model_0 = Pipeline([\n",
        "    (\"tf-idf\", TfidfVectorizer()),\n",
        "    (\"clf\", MultinomialNB())\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model_0.fit(train_sentences,\n",
        "            train_labels_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "kksnVb5Ap9rT",
        "outputId": "3e1321f5-b05b-4336-b643-688b01d9f74e"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-188-d453d2934fbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Fit the pipeline to the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m model_0.fit(train_sentences,\n\u001b[0m\u001b[1;32m     13\u001b[0m             train_labels_encoded)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \"\"\"\n\u001b[1;32m    389\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    349\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2075\u001b[0m         \"\"\"\n\u001b[1;32m   2076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1206\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate baseline model on validation dataset\n",
        "model_0.score(val_sentences,\n",
        "              val_labels_encoded)"
      ],
      "metadata": {
        "id": "wkMbGTMAqjRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions using our baseline model\n",
        "baseline_preds = model_0.predict(val_sentences)\n",
        "baseline_preds"
      ],
      "metadata": {
        "id": "yfNxnn7Lqp-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download helper functions script\n",
        "\n",
        "In the previous module, we wrote a function to compare predictions across different metrics (accuracy, precision, recall and f1) and rather than rewriting it here, let's download it from our hlper function script"
      ],
      "metadata": {
        "id": "eqWt83hhq3bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
      ],
      "metadata": {
        "id": "4d2AQW_Xq6Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import calculate_results"
      ],
      "metadata": {
        "id": "8EtxFvlzrvxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate baseline results\n",
        "baseline_results = calculate_results(val_labels_encoded,\n",
        "                                     baseline_preds)\n",
        "baseline_results"
      ],
      "metadata": {
        "id": "bhwTAojPrzZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences[:10]"
      ],
      "metadata": {
        "id": "iHFsZlDLr6PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing our data (the text) for deep sequence models\n",
        "\n",
        "Before we start building deeper models, we've got to create vectorization and embedding layers."
      ],
      "metadata": {
        "id": "Qyazl8--sH3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "dbbu7tcWsnVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How long is each sentence on average?\n",
        "sent_lens = [len(sentence.split()) for sentence in train_sentences]\n",
        "avg_sent_len = np.mean(sent_lens)\n",
        "avg_sent_len"
      ],
      "metadata": {
        "id": "uLCn4fyLsqlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What's the distribution look like\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(sent_lens, bins=20)"
      ],
      "metadata": {
        "id": "6thn2iFet1zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How long of a sentence length covers 95% of examples?\n",
        "output_seq_len = int(np.percentile(sent_lens, 95))\n",
        "output_seq_len"
      ],
      "metadata": {
        "id": "qRTcFxexuwRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create text vectorizer layer\n",
        "\n",
        "We wanto to make a layer which maps our text words in numbers."
      ],
      "metadata": {
        "id": "Ap1jZqqkv3up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many words are in our vocab? (taken from table 2 in the baseline paper) \n",
        "max_tokens = 68000"
      ],
      "metadata": {
        "id": "kYgMCtjuxGmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create text vectorizer\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=max_tokens, # number of words in vocab\n",
        "                                    output_sequence_length=output_seq_len) # desired output length of vectorized sequences"
      ],
      "metadata": {
        "id": "r4G3mxN4yA96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt text vectorizer to training sentences\n",
        "text_vectorizer.adapt(train_sentences)"
      ],
      "metadata": {
        "id": "qewEwy5LyFwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out text vectorizer on random sentences\n",
        "import random\n",
        "target_sentence = random.choice(train_sentences)\n",
        "print(f\"Text:\\n{target_sentence}\")\n",
        "print(f\"\\nLength of text: {len(target_sentence.split())}\")\n",
        "print(f\"\\nVectorized text: {text_vectorizer([target_sentence])}\")"
      ],
      "metadata": {
        "id": "vgvI1FG3ytkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many words in our training vocabulary \n",
        "rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
        "print(f\"Number of words in vocab: {len(rct_20k_text_vocab)}\")\n",
        "print(f\"Most common words in the vocab: {rct_20k_text_vocab[:5]}\")\n",
        "print(f\"Least common words in the vocab: {rct_20k_text_vocab[-5:]}\")"
      ],
      "metadata": {
        "id": "6j2sSqoMzHIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the config of our text vecotirzzer\n",
        "text_vectorizer.get_config()"
      ],
      "metadata": {
        "id": "C9VqGZ7-z4eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create custom text embedding "
      ],
      "metadata": {
        "id": "euD0A5u10Fcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create token embedding layer\n",
        "token_embed = layers.Embedding(input_dim = len(rct_20k_text_vocab),\n",
        "                               output_dim=128, # Note: different embedding sizes result in drastically different parameters to train\n",
        "                               mask_zero=True, # use masking to handle variable sequence lengths (save space)\n",
        "                               name=\"token_embedding\")"
      ],
      "metadata": {
        "id": "VA7Wc5G80JWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show example embedding\n",
        "print(f\"Sentence before vectorization:\\n {target_sentence}\\n\")\n",
        "vectorized_sentence = text_vectorizer([target_sentence])\n",
        "print(f\"Sentence after vectorization (before embedding):\\n {vectorized_sentence}\\n\")\n",
        "embedded_sentence = token_embed(vectorized_sentence)\n",
        "print(f\"Sentence after embedding:\\n {embedded_sentence}\\n\")\n",
        "print(f\"Embedded sentence shape: {embedded_sentence.shape}\")"
      ],
      "metadata": {
        "id": "z27zQv8H1QfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating datasets (making sure our data loads as fast as possible)\n",
        "\n",
        "We're going to setup our data to run as fast as possible with the TensorFlow tf.data API, many of the steps hare are discussed at length in these two resources:\n",
        "* https://www.tensorflow.org/guide/data_performance\n",
        "* https://www.tensorflow.org/guide/data"
      ],
      "metadata": {
        "id": "Jeo6HepE128r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn our data into TensorFlow Datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
        "\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "24rwnxH13POd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the TensorSliceDataset's and turn them into prefetch datasets\n",
        "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "qsmxl0rh3qvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: Conv1D with token embeddings"
      ],
      "metadata": {
        "id": "c8YvxCpE31Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 1D conv model to process sequences\n",
        "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
        "text_vectors = text_vectorizer(inputs) # vectorize text inputs\n",
        "token_embeddings = token_embed(text_vectors) # create embedding\n",
        "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(token_embeddings)\n",
        "x = layers.GlobalAveragePooling1D()(x) # condense the output of our feature vector from conv layer\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "model_1 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile \n",
        "model_1.compile(loss=\"CategoricalCrossentropy\",\n",
        "                optimizer=\"adam\",\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "model_1.summary()"
      ],
      "metadata": {
        "id": "UbPYEqbK5Xcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "history_model_1 = model_1.fit(train_dataset,\n",
        "                              #steps_per_epoch=int(0.1*len(train_dataset)),\n",
        "                              epochs=3,\n",
        "                              validation_data=valid_dataset,\n",
        "                              #validation_steps=int(0.1 * len(valid_dataset)) # only validate on 10% of the valid data\n",
        "                              ) "
      ],
      "metadata": {
        "id": "9tAB--036ayT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on whole validation dataset\n",
        "model_1.evaluate(valid_dataset)"
      ],
      "metadata": {
        "id": "9s-mAEwQ7b4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions (our model predicts prediction probabilities for each class)\n",
        "model_1_pred_probs = model_1.predict(valid_dataset)\n",
        "model_1_pred_probs, model_1_pred_probs.shape"
      ],
      "metadata": {
        "id": "kwHuZG0r8YFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert pred probs to classes\n",
        "model_1_preds = tf.argmax(model_1_pred_probs, axis=1)\n",
        "model_1_preds"
      ],
      "metadata": {
        "id": "JZg3dbaF8kul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate model_1 results\n",
        "model_1_results = calculate_results(val_labels_encoded,\n",
        "                                    model_1_preds)\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "33VrsGYj9FwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results"
      ],
      "metadata": {
        "id": "X1Ek6xUc9PkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: Feature extraction with pretrained token embeddings\n",
        "\n",
        "Now let's use pretrained word embeddings from TensorFlow Hub, more specifcally the universal sentece encoder: https://tfhub.dev/google/universal-sentence-encoder/4\n",
        "\n",
        "The paper originally used GloVe megedding, however, we're going to stick with later created USE pre trained embeddings"
      ],
      "metadata": {
        "id": "FFXeAzeW9R7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pretrained TensorFlow Hub USE\n",
        "import tensorflow_hub as hub\n",
        "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                                        trainable=False,\n",
        "                                        name=\"universal_sentence_encoder\")"
      ],
      "metadata": {
        "id": "OfTQdpngLDIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out the pretrained embedding on a random sentence\n",
        "random_train_sentence = random.choice(train_sentences)\n",
        "print(f\"Random sentence:\\n{random_train_sentence}\\n\")\n",
        "use_embedded_sentence = tf_hub_embedding_layer([random_train_sentence])\n",
        "print(f\"Sentence after embedding:\\n{use_embedded_sentence[0][:20]}\\n\")\n",
        "print(f\"Length of sentence embedding: {len(use_embedded_sentence[0])}\")"
      ],
      "metadata": {
        "id": "8sqVA1wZM15G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building and fitting an NLP feature extraction model using pretrained embeddings TensorFlow Hub"
      ],
      "metadata": {
        "id": "usZH77nhNTGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature extraction model using TF Hub layer\n",
        "inputs = layers.Input(shape=[], dtype=tf.string)\n",
        "pretrained_embedding = tf_hub_embedding_layer(inputs) # tokenize text and create embedding of each sequence (512 long vector)\n",
        "x = layers.Dense(128, activation=\"relu\")(pretrained_embedding)\n",
        "# Note: you could add more layers here if you wanted to\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x) # create the output layer\n",
        "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_USE_feature_extractor\")\n",
        "\n",
        "# Compile\n",
        "model_2.compile(loss=\"CategoricalCrossentropy\",\n",
        "                optimizer=\"adam\",\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "model_2.summary()"
      ],
      "metadata": {
        "id": "gjGgN789N26e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model_2 to the data\n",
        "history_model_2 = model_2.fit(train_dataset,\n",
        "                              epochs=3,\n",
        "                              steps_per_epoch=int(0.1 * len(train_dataset)),\n",
        "                              validation_data=valid_dataset,\n",
        "                              validation_steps=int(0.1 * len(valid_dataset)))"
      ],
      "metadata": {
        "id": "eEKDTb3UOsfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the whole validation dataset\n",
        "model_2.evaluate(valid_dataset)"
      ],
      "metadata": {
        "id": "yjw2EycqPIzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with feature extraction model\n",
        "model_2_preds_probs = model_2.predict(valid_dataset)"
      ],
      "metadata": {
        "id": "WcF_Zb_cPcQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conver the predictions probabilities found with feature extraction model to labels\n",
        "model_2_preds = tf.argmax(model_2_preds_probs, axis=1)\n",
        "model_2_preds\n"
      ],
      "metadata": {
        "id": "_xf_4nJVPlMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate results from TF Hub pretrained embeddings results on val set\n",
        "model_2_results = calculate_results(val_labels_encoded,\n",
        "                                    model_2_preds)\n",
        "model_2_results"
      ],
      "metadata": {
        "id": "LG3NQKvmPx6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_results"
      ],
      "metadata": {
        "id": "E0ZdEoWEP7tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3: Conv1D with character embeddings\n",
        "\n",
        "The paper which we're replicating states they used a combination of token and character-level mebddings.\n",
        "\n",
        "Previously we've made token-level embeddings but we'll need to do similar steps for character if we want to use char-level embeddings."
      ],
      "metadata": {
        "id": "BN0dgo-pP_P6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a character-lebel tokenizer"
      ],
      "metadata": {
        "id": "hR9jndaYQm3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_characters = []\n",
        "for sentence in train_sentences:\n",
        "    for letter in sentence:\n",
        "        train_characters.append(letter)\n",
        "\n",
        "print(train_characters[:50])"
      ],
      "metadata": {
        "id": "O8_YdoPPRHn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make function to split sentences into characters\n",
        "def split_chars(text):\n",
        "    return \" \".join(list(text))\n",
        "\n",
        "# Text splitting non-character-level sequence into characters\n",
        "split_chars(random_train_sentence)"
      ],
      "metadata": {
        "id": "5VEpR2D4RMaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split sequence-level data splits into character-level data splits\n",
        "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
        "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
        "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
        "\n",
        "train_chars[:5]"
      ],
      "metadata": {
        "id": "wB3U88rYRxAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What's the average character length?\n",
        "char_lens = [len(sentence) for sentence in train_sentences]\n",
        "mean_char_len = np.mean(char_lens)\n",
        "mean_char_len"
      ],
      "metadata": {
        "id": "CktJsGbJSLOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of our sequences at a character-level\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(char_lens, bins=7)"
      ],
      "metadata": {
        "id": "xBHmRt_fSknR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find what character length covers 95% of sequences\n",
        "output_seq_char_len = int(np.percentile(char_lens, 95))\n",
        "output_seq_char_len"
      ],
      "metadata": {
        "id": "RTiNxsEpS_KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all keyboard characters\n",
        "import string\n",
        "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
        "alphabet"
      ],
      "metadata": {
        "id": "rjGyAVhuTMH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create char-level token vectorizer instance\n",
        "NUM_CHAR_TOKENS = len(alphabet) + 2 # add 2 for space and OOV token (OOV = out of vocab, '[UNK]')\n",
        "char_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS,\n",
        "                                    output_sequence_length=output_seq_char_len,\n",
        "                                    # standardize=\"lower_and_strip_punctuation\",\n",
        "                                    name=\"char_vectorizer\")"
      ],
      "metadata": {
        "id": "MCLd2NvnTkcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt character vectorizer to training character\n",
        "char_vectorizer.adapt(train_chars)"
      ],
      "metadata": {
        "id": "n1xX7AopUdg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check character vocab stats\n",
        "char_vocab = char_vectorizer.get_vocabulary()\n",
        "print(f\"Number of different characters in character vocab: {len(char_vocab)}\")\n",
        "print(f\"5 most common characters: {char_vocab[:5]}\")\n",
        "print(f\"5 least common characters: {char_vocab[-5:]}\")"
      ],
      "metadata": {
        "id": "kr6NDeHQUmoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out character vectorizer\n",
        "random_train_chars = random.choice(train_chars)\n",
        "print(f\"Charified text:\\n{random_train_chars}\\n\")\n",
        "print(f\"Length of random_train_chars: {len(random_train_chars.split())}\")\n",
        "vectorized_chars = char_vectorizer([random_train_chars])\n",
        "print(f\"\\nVectorized chars:\\n {vectorized_chars}\")\n",
        "print(f\"\\nLength of vectorized chars: {len(vectorized_chars[0])}\")"
      ],
      "metadata": {
        "id": "t9AaHUFaUr_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a character-level embedding"
      ],
      "metadata": {
        "id": "3IfzBVKjVzQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create char embedding layer\n",
        "char_embed = layers.Embedding(input_dim=len(char_vocab), # number of characters\n",
        "                              output_dim=25, # this is the size of the char embedding in the paper\n",
        "                              mask_zero=True,\n",
        "                              name=\"char_embed\")"
      ],
      "metadata": {
        "id": "C2UGItvZUVwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test our character embedding layer\n",
        "print(f\"Charified text:\\n {random_train_chars}\\n\")\n",
        "char_embed_example = char_embed(char_vectorizer([random_train_chars]))\n",
        "print(f\"Embedded chars. (after vectorization and embedding):\\n{char_embed_example}\")\n",
        "print(f\"Character embedding shape: {char_embed_example.shape}\")"
      ],
      "metadata": {
        "id": "ofZ3DfGrU44T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(random_train_chars)"
      ],
      "metadata": {
        "id": "JbU7LrIUVSIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a Conv1D model to fit on character embeddings"
      ],
      "metadata": {
        "id": "TcgGU7KnWAsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Conv1D on chars only\n",
        "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
        "char_vectors = char_vectorizer(inputs)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(char_embeddings)\n",
        "x = layers.GlobalMaxPool1D()(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "model_3 = tf.keras.Model(inputs=inputs,\n",
        "                         outputs=outputs,\n",
        "                         name=\"model_3_conv1d_char_embeddings\")\n",
        "\n",
        "# Compile\n",
        "model_3.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "model_3.summary()"
      ],
      "metadata": {
        "id": "V5I3nxcuVcqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create char level datasets\n",
        "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_char_dataset = tf.data.Dataset.from_tensor_slices((test_chars, test_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "train_char_dataset"
      ],
      "metadata": {
        "id": "38EvtvQaW3yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model on chars only\n",
        "model_3_history = model_3.fit(train_char_dataset,\n",
        "                              epochs=5,\n",
        "                              steps_per_epoch=int(0.1*len(train_char_dataset)),\n",
        "                              validation_data=val_char_dataset,\n",
        "                              validation_steps=int(0.1*len(val_char_dataset)))"
      ],
      "metadata": {
        "id": "Lfd6Pu3EXhtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with character model only\n",
        "model_3_pred_probs = model_3.predict(val_char_dataset)\n",
        "model_3_pred_probs"
      ],
      "metadata": {
        "id": "6QusXFmKX2G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3_preds = tf.argmax(model_3_pred_probs, axis=1)\n",
        "model_3_preds"
      ],
      "metadata": {
        "id": "OsSci6FcX80A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate results for Conv1D model chars only\n",
        "model_3_results = calculate_results(val_labels_encoded,\n",
        "                                    model_3_preds)\n",
        "model_3_results"
      ],
      "metadata": {
        "id": "6EcFobG8Yxwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results"
      ],
      "metadata": {
        "id": "CDVUMmzaY6C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Model 4: Combining pretrianed token embeddings + characters embeddings (hybrid embedding layer)\n",
        "\n",
        " 1. Create a token-level embedding model (similar to `model_1`)\n",
        " 2. Create a character-level model (similar to `model_3` with a slight modification)\n",
        " 3. Combine 1 & 2 with a concatenate (`layers.Concatenate`)\n",
        " 4. Build a series of output layers on top of 3 similar to Figure 1 and section 4.2 of the paper\n",
        " 5. Construct a model which takes token and character-level sequences as input and produces sequence label probabilities as input"
      ],
      "metadata": {
        "id": "5-LO2JIdY-xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup token inputs/model\n",
        "token_inputs = layers.Input(shape=[], dtype=\"string\", name=\"token_input\")\n",
        "token_embedding = tf_hub_embedding_layer(token_inputs)\n",
        "token_output = layers.Dense(128, activation=\"relu\")(token_embedding)\n",
        "token_model = tf.keras.Model(token_inputs,\n",
        "                             token_output)\n",
        "\n",
        "# 2. Setup char inputs/model\n",
        "char_inputs = layers.Input(shape=(1,), dtype=\"string\", name=\"char_input\")\n",
        "char_vectors = char_vectorizer(char_inputs)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "char_bi_lstm = layers.Bidirectional(layers.LSTM(24))(char_embeddings) # bi-LSTM shown in figure 1 of the paper\n",
        "char_model = tf.keras.Model(char_inputs,\n",
        "                            char_bi_lstm)\n",
        "\n",
        "# 3. Concatenate token and char inputs (create hybrid token embedding)\n",
        "token_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output, char_model.output])\n",
        "\n",
        "# 4. Create output layers - adding in Dropout, discussed in section 4.2\n",
        "combined_dropout = layers.Dropout(0.5)(token_char_concat)\n",
        "combined_dense = layers.Dense(128, activation=\"relu\")(combined_dropout)\n",
        "final_dropout = layers.Dropout(0.5)(combined_dense) \n",
        "output_layer = layers.Dense(num_classes, activation=\"softmax\")(final_dropout)\n",
        "\n",
        "# 5. Construct model iwth char and token inputs\n",
        "model_4 = tf.keras.Model(inputs=[token_model.input, char_model.input],\n",
        "                         outputs=output_layer,\n",
        "                         name=\"model_4_token_and_char_embeddings\")\n"
      ],
      "metadata": {
        "id": "G91bb5rOa3Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a summary of our model\n",
        "model_4.summary()"
      ],
      "metadata": {
        "id": "uTSzWJMlbtdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot hybrid token and character model\n",
        "from keras.utils import plot_model\n",
        "plot_model(model_4, show_shapes=True)"
      ],
      "metadata": {
        "id": "M3XUj7RKerll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile token charn model\n",
        "model_4.compile(loss=\"CategoricalCrossentropy\",\n",
        "                optimizer=\"adam\", # section 4.2 says they use SGD\n",
        "                metrics=\"accuracy\")"
      ],
      "metadata": {
        "id": "L3atvi_bfvQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining token and character data into a tf.data Dataset\n"
      ],
      "metadata": {
        "id": "KosIRHdBh8d7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine chars and tokens into a dataset\n",
        "train_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars)) # make data\n",
        "train_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # make labels\n",
        "train_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels)) # combine data and labels\n",
        "\n",
        "# Prefetch and batch train data\n",
        "train_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "J8QzywJoiEA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repeat the above steps for our validation data\n",
        "val_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars)) # make data\n",
        "val_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot) # make labels\n",
        "val_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, val_char_token_labels)) # combine data and labels\n",
        "\n",
        "# Prefetch and batch train data\n",
        "val_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "bxmudnAJhayM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out our training char and token embedding dataset\n",
        "train_char_token_dataset, val_char_token_dataset"
      ],
      "metadata": {
        "id": "mCiU8m-QjX8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitting a model on token and character-level sequences"
      ],
      "metadata": {
        "id": "1qjPMExtjjF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model on tokens and chars\n",
        "history_model_4 = model_4.fit(train_char_token_dataset,\n",
        "                              steps_per_epoch=int(0.1 * len(train_char_token_dataset)),\n",
        "                              epochs=3,\n",
        "                              validation_data=val_char_token_dataset,\n",
        "                              validation_steps=int(0.1*len(val_char_token_dataset)))"
      ],
      "metadata": {
        "id": "Wx78c0qhkB5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the whole validation dataset\n",
        "model_4.evaluate(val_char_token_dataset)"
      ],
      "metadata": {
        "id": "kicOAaFukJT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions using the token-character model hybrid\n",
        "model_4_pred_probs = model_4.predict(val_char_token_dataset)"
      ],
      "metadata": {
        "id": "p8QPOO2Alm3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format pred probs into pred labels\n",
        "model_4_preds = tf.argmax(model_4_pred_probs, axis=1)\n",
        "model_4_preds"
      ],
      "metadata": {
        "id": "b186Q_T4l4iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate results\n",
        "model_4_results = calculate_results(val_labels_encoded,\n",
        "                                    model_4_preds)\n",
        "model_4_results"
      ],
      "metadata": {
        "id": "XTqbzsHymELb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 5: Transfer learning with pretrained token embeddings + character embeddings + positional embeddings"
      ],
      "metadata": {
        "id": "REiszGInmLA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "WBfuDdPAme8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:** Any engineered features used to train a model need to be available at test time."
      ],
      "metadata": {
        "id": "PsZZU-0zoqWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create positional embeddings"
      ],
      "metadata": {
        "id": "vFY34mWxp-8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many different line numbers are there?\n",
        "train_df[\"line_number\"].value_counts()"
      ],
      "metadata": {
        "id": "nk70qHsoqMXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of \"line_number\" column\n",
        "train_df.line_number.plot.hist()"
      ],
      "metadata": {
        "id": "2MjonS5_qXgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use TensorFlow to create one-hot-encoded tensors of our \"line-number\" column\n",
        "train_line_numbers_one_hot = tf.one_hot(train_df[\"line_number\"].to_numpy(), depth=15)\n",
        "val_line_numbers_one_hot = tf.one_hot(val_df[\"line_number\"].to_numpy(), depth=15)\n",
        "test_line_numbers_one_hot = tf.one_hot(test_df[\"line_number\"].to_numpy(), depth=15)\n",
        "train_line_numbers_one_hot.shape"
      ],
      "metadata": {
        "id": "A8sIj10jqyCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've encoded our line numbers feature, let's do the same for our total lines feature..."
      ],
      "metadata": {
        "id": "kVfOmqiPrKcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many different numbers of lines are there?\n",
        "train_df[\"total_lines\"].value_counts()"
      ],
      "metadata": {
        "id": "tgNcOKbEsPWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.total_lines.plot.hist()"
      ],
      "metadata": {
        "id": "9HtIAHD9sT3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the coverage of a \"total_lines\" value of 20\n",
        "np.percentile(train_df.total_lines, 98)"
      ],
      "metadata": {
        "id": "HEQ-UgMlsgSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use TensorFlow to create one-hot-encoded tensors of our \"total_lines\" feature\n",
        "train_total_lines_one_hot = tf.one_hot(train_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "val_total_lines_one_hot = tf.one_hot(val_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "test_total_lines_one_hot = tf.one_hot(test_df[\"total_lines\"].to_numpy(), depth=20)"
      ],
      "metadata": {
        "id": "o0j-1puFs_I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a tribrid embedding model\n",
        "\n",
        "1. Create a token-level model\n",
        "2. Create a character-level model\n",
        "3. Create a model for the \"line_number\" feature\n",
        "4. Create a model for the \"total_lines\" feature\n",
        "5. Combine the outputs of 1 & 2 using tf.keras.layers.Concatenate\n",
        "6. Combine the outputs of 3, 4, 5 using tf.keras.layers.Concatenate\n",
        "7. Create an output layer to accept the tribried embedding and output label probabilities\n",
        "8. Combine the inputs 1, 2, 3, 4 and outputs of 7 into a tf.keras.Model"
      ],
      "metadata": {
        "id": "aoNvTKogtSXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Token inputs\n",
        "token_inputs = layers.Input(shape=[], dtype=\"string\", name=\"token_inputs\")\n",
        "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
        "token_outputs = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
        "token_model = tf.keras.Model(inputs=token_inputs,\n",
        "                             outputs=token_outputs)\n",
        "\n",
        "# 2. Char inputs\n",
        "char_inputs = layers.Input(shape=(1,), dtype=\"string\", name=\"char_inputs\")\n",
        "char_vectors = char_vectorizer(char_inputs)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "char_bi_lstm = layers.Bidirectional(layers.LSTM(24))(char_embeddings)\n",
        "char_model = tf.keras.Model(inputs=char_inputs,\n",
        "                            outputs=char_bi_lstm)\n",
        "\n",
        "# 3. Line numbers\n",
        "line_number_inputs = layers.Input(shape=(15), dtype=tf.float32, name=\"line_number_input\")\n",
        "line_dense = layers.Dense(32, activation=\"relu\")(line_number_inputs)\n",
        "line_number_model = tf.keras.Model(line_number_inputs,\n",
        "                            line_dense)\n",
        "\n",
        "# 4. Total Line\n",
        "total_lines_inputs = layers.Input(shape=(20), dtype=tf.float32, name=\"total_line_input\")\n",
        "total_lines_dense = layers.Dense(32, activation=\"relu\")(total_lines_inputs)\n",
        "total_lines_model = tf.keras.Model(total_lines_inputs,\n",
        "                                  total_lines_dense)\n",
        "\n",
        "# 5. Combine token and char embeddings into a hybrid embedding\n",
        "combined_embeddings = layers.Concatenate(name=\"char_token_hybrid_embedding\")([token_model.output,\n",
        "                                                                              char_model.output])\n",
        "\n",
        "z = layers.Dense(256, activation=\"relu\")(combined_embeddings)\n",
        "z = layers.Dropout(0.5)(z)\n",
        "\n",
        "\n",
        "# 6. Combine positional embedding with combined token and char embeddings\n",
        "tribrid_embeddings = layers.Concatenate(name=\"char_token_positional_embedding\")([line_number_model.output,\n",
        "                                                                                 total_lines_model.output,\n",
        "                                                                                 z])\n",
        "\n",
        "# 7. Create output layer\n",
        "output_layer = layers.Dense(5, activation=\"softmax\", name=\"output_layer\")(tribrid_embeddings)\n",
        "\n",
        "# 8. Put together model with all kinds of inputs\n",
        "model_5 = tf.keras.Model(inputs=[line_number_model.input,\n",
        "                                 total_lines_model.input,\n",
        "                                 token_model.input,\n",
        "                                 char_model.input\n",
        "                                 ],\n",
        "                         outputs=output_layer,\n",
        "                         name=\"model_5_tribrid_embedding_model\")"
      ],
      "metadata": {
        "id": "I9MMo98puxVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a summary of our tribrid model\n",
        "model_5.summary()"
      ],
      "metadata": {
        "id": "UPa_4GUdw8JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot model 5\n",
        "from keras.utils import plot_model\n",
        "plot_model(model_5)"
      ],
      "metadata": {
        "id": "66w1Vb_voa5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is label smoothing?\n",
        "\n",
        "For example, if our model gets too confident on a single class (e.g. its prediction probability is really high), it may get stuck on that class and not \n",
        "consider other classes...\n",
        "\n",
        "Really confident: `[0.0, 0.0, 1.0, 0.0, 0.0]`\n",
        "\n",
        "What label smotthing does is it assigns some of the value from the highest pred prob to other classes, in turn, hopefully improving generalization: `[0.01, 0.01, 0.96, 0.01, 0.01]`\n"
      ],
      "metadata": {
        "id": "EZ9dT9K3qBzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile token, char and positional embedding model\n",
        "model_5.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2), # helps to prevent overfitting\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=\"accuracy\")"
      ],
      "metadata": {
        "id": "XJzDtzLso536"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create tribrid embedding datasets using tf.data"
      ],
      "metadata": {
        "id": "pYtl_fvYrMcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation datasets (with all four kinds of input data)\n",
        "train_char_token_pos_data = tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot,\n",
        "                                                                train_total_lines_one_hot,\n",
        "                                                                train_sentences,\n",
        "                                                                train_chars))\n",
        "train_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
        "train_char_token_pos_dataset = tf.data.Dataset.zip((train_char_token_pos_data, train_char_token_pos_labels))\n",
        "train_char_token_pos_dataset = train_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Do the same as above for val dataset\n",
        "val_char_token_pos_data = tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot,\n",
        "                                                              val_total_lines_one_hot,\n",
        "                                                              val_sentences,\n",
        "                                                              val_chars))\n",
        "val_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
        "val_char_token_pos_dataset = tf.data.Dataset.zip((val_char_token_pos_data, val_char_token_pos_labels))\n",
        "val_char_token_pos_dataset = val_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "N_OHx0G5rV-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check input shapes\n",
        "train_char_token_pos_dataset, val_char_token_pos_dataset"
      ],
      "metadata": {
        "id": "gvmYFpiwsvdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitting, evaluating and making predictions with out tribrid model"
      ],
      "metadata": {
        "id": "FoKCAoI_tK5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "history_model_5 = model_5.fit(train_char_token_pos_dataset,\n",
        "                              epochs=5,\n",
        "                              steps_per_epoch=int(0.1 * len(train_char_token_pos_dataset)),\n",
        "                              validation_data=val_char_token_pos_dataset,\n",
        "                              validation_steps=int(0.1 * len(val_char_token_pos_dataset)))"
      ],
      "metadata": {
        "id": "qRVnbFqRtikp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with the char token pos model\n",
        "model_5_pred_probs = model_5.predict(val_char_token_pos_dataset, verbose=1)\n",
        "model_5_pred_probs"
      ],
      "metadata": {
        "id": "dgByC7a5t5b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert pred probs to pred labels\n",
        "model_5_preds = tf.argmax(model_5_pred_probs, axis=1)\n",
        "model_5_preds"
      ],
      "metadata": {
        "id": "w5dC2JRguXMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate results of char token pos model\n",
        "model_5_results = calculate_results(val_labels_encoded,\n",
        "                                    model_5_preds)\n",
        "model_5_results"
      ],
      "metadata": {
        "id": "dOtZG8Swuce4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare model results\n"
      ],
      "metadata": {
        "id": "cHSxGJ1uvAtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine model results into a dataframe\n",
        "all_model_results = pd.DataFrame({\"model_0_baseline\": baseline_results,\n",
        "                                  \"model_1_custom_token\": model_1_results,\n",
        "                                  \"model_2_pretrained_token_embedding\": model_2_results,\n",
        "                                  \"model_3_custom_char_embedding\": model_3_results,\n",
        "                                  \"model_4_hybrid_char_token_embedding\": model_4_results,\n",
        "                                  \"model_5_pos_char_token_embedding\": model_5_results})\n",
        "all_model_results = all_model_results.transpose()\n",
        "all_model_results"
      ],
      "metadata": {
        "id": "ifQA36lcvEIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce the accuracy to same scale as other metrics\n",
        "all_model_results[\"accuracy\"] = all_model_results[\"accuracy\"]/100"
      ],
      "metadata": {
        "id": "9TRCZriRwxEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot and compare all model results\n",
        "all_model_results.plot(kind=\"bar\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0))"
      ],
      "metadata": {
        "id": "oE56usFpxS-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort models results by f1-score\n",
        "all_model_results.sort_values(\"f1\", ascending=False)[\"f1\"].plot(kind=\"bar\", figsize=(10,7))"
      ],
      "metadata": {
        "id": "kICy3KfGxayb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and load model"
      ],
      "metadata": {
        "id": "NaSuZrqIx5JP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best performing model to SavedModel format (default)\n",
        "model_5.save(\"smilit_tribrid_model\")"
      ],
      "metadata": {
        "id": "GEn5DdXHyWa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in best performing model\n",
        "loaded_model = tf.keras.models.load_model(\"smilit_tribrid_model\")"
      ],
      "metadata": {
        "id": "PNz3oqiKydnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with the loaded model on the validation set\n",
        "loaded_pred_probs = loaded_model.predict(val_char_token_pos_dataset)\n",
        "loaded_preds = tf.argmax(loaded_pred_probs, axis=1)"
      ],
      "metadata": {
        "id": "PJhu7QBIzMLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the results of our loaded model\n",
        "loaded_model_results = calculate_results(val_labels_encoded,\n",
        "                                         loaded_preds)\n",
        "loaded_model_results"
      ],
      "metadata": {
        "id": "kQCUhX9izhzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_5_results == loaded_model_results"
      ],
      "metadata": {
        "id": "ssoHI3iQzsvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your challenges\n",
        "\n",
        "1. Turn the test data samples into a tf.data Dataset (fast loading) and then evaluate (make predictions) the best performing model on the test labels.\n",
        "2. Find the most wrong predictions from 1 (these are the samples where the model has predicted the wrong label with the highest prediction probability).\n",
        "3. Make example predictions (on RCT abstracts from the wild), you can go to PubMed to find these: https://pubmed.ncbi.nlm.nih.gov/, find and use our model to predict on 3-4 different abstracts from the wild.\n",
        "  * Some examples: https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/skimlit_example_abstracts.json\n",
        "4. See the exercises and extra curriculum section on GitHub to further test your skills (for section 09)"
      ],
      "metadata": {
        "id": "49Rf0GRD1yNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Milestone 2: SkimLit\n"
      ],
      "metadata": {
        "id": "s6B00KJk2VcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goals\n",
        "\n",
        "1. Turn the test data samples into a tf.data Dataset (fast loading) and then evaluate (make predictions) the best performing model on the test labels.\n",
        "2. Find the most wrong predictions from 1 (these are the samples where the model has predicted the wrong label with the highest prediction probability).\n",
        "3. Make example predictions (on RCT abstracts from the wild), you can go to PubMed to find these: https://pubmed.ncbi.nlm.nih.gov/, find and use our model to predict on 3-4 different abstracts from the wild.\n",
        "  * Some examples: https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/skimlit_example_abstracts.json\n",
        "4. See the exercises and extra curriculum section on GitHub to further test your skills (for section 09)"
      ],
      "metadata": {
        "id": "ibw3KxFrvj2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting our Data"
      ],
      "metadata": {
        "id": "wgWRr49Fv8b6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data\n",
        "# !git clone https://github.com/Franck-Dernoncourt/pubmed-rct\n",
        "# !ls pubmed-rct"
      ],
      "metadata": {
        "id": "KeP1Nza2vtpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Separate our data properly\n",
        "test_data = pd.DataFrame(preprocess_text_with_line_numbers(\"/content/pubmed-rct/PubMed_200k_RCT_numbers_replaced_with_at_sign/test.txt\"))\n",
        "test_sentences = test_data[\"text\"]\n",
        "test_labels = test_data[\"target\"]\n",
        "test_line_number = test_data[\"line_number\"]\n",
        "test_total_lines = test_data[\"total_lines\"]\n",
        "test_chars = list(test_sentences)\n",
        "test_data.head()"
      ],
      "metadata": {
        "id": "sSScSd7Tvx_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract labels (\"target\" columns) and encode them into integers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "test_labels_encoded = label_encoder.fit_transform(test_labels.to_numpy())\n",
        "\n",
        "# Check what training labels look like\n",
        "test_labels_encoded"
      ],
      "metadata": {
        "id": "ejqx5rG3-Tt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels.value_counts()"
      ],
      "metadata": {
        "id": "XvXKItpW557r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# One hot encode line number, labels and total lines variables\n",
        "one_hot = OneHotEncoder(sparse=False)\n",
        "test_labels_one_hot = one_hot.fit_transform(test_labels.to_numpy().reshape(-1, 1))\n",
        "test_line_number_one_hot = tf.one_hot(test_line_number.to_numpy(), depth=15)\n",
        "test_total_lines_one_hot = tf.one_hot(test_total_lines.to_numpy(), depth=20)\n",
        "\n",
        "# Create a tf.dataset for our data\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_line_number_one_hot,\n",
        "                                                test_total_lines_one_hot,\n",
        "                                                test_sentences, \n",
        "                                                test_chars))\n",
        "test_data_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
        "test_dataset = tf.data.Dataset.zip((test_data, test_data_labels))\n",
        "\n",
        "# Prefetch it for better performance\n",
        "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset"
      ],
      "metadata": {
        "id": "_4Km_i7vv0CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "PJ2hwsrlnZ4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Turn the test data samples into a tf.data Dataset (fast loading) and then *evaluate* (make predictions) the best performing model on the test labels."
      ],
      "metadata": {
        "id": "7F66TSKNwA-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and get the prediction array\n",
        "skim_preds_probs = model_5.predict(test_dataset)\n",
        "skim_preds_probs"
      ],
      "metadata": {
        "id": "G7NxRxK4wDT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will be useful later\n",
        "skim_preds_max_prob = []\n",
        "for i, prob in enumerate(skim_preds_probs):\n",
        "    skim_preds_max_prob.append(float(tf.reduce_max(skim_preds_probs[i])))\n",
        "len(skim_preds_max_prob)\n"
      ],
      "metadata": {
        "id": "djVvfEhRqhr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn our prediction probabilities into an index of the class predicted\n",
        "skim_preds = tf.argmax(skim_preds_probs, axis=1)\n",
        "skim_preds"
      ],
      "metadata": {
        "id": "-8o8UiuLwSKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the results using our previous defined function\n",
        "calculate_results(test_labels_encoded,\n",
        "                  skim_preds)"
      ],
      "metadata": {
        "id": "6uQbAHVj7-uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now to fulfill the second goal, we need a dictionary containing the informations of our predictions\n",
        "pred_dic = {\"sentence\": test_sentences,\n",
        "            \"target_class\": class_names[test_labels_encoded],\n",
        "            \"predicted_class\": class_names[skim_preds],\n",
        "            \"predicted_prob\": skim_preds_max_prob}\n",
        "pred_dic = pd.DataFrame(pred_dic)\n",
        "pred_dic.head()"
      ],
      "metadata": {
        "id": "V15E6KXa_dN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we just need a df with only the wrong predictions\n",
        "wrong_preds = pred_dic[pred_dic[\"target_class\"] != pred_dic[\"predicted_class\"]].sort_values(\"predicted_prob\", ascending=False)\n",
        "\n",
        "# Done, now we have all the information we need to check our most wrong predictions (higher predicted_prob, for the wrong predicted_class)\n",
        "wrong_preds.head(20)"
      ],
      "metadata": {
        "id": "k36PMie3tJbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making prediction on outside data"
      ],
      "metadata": {
        "id": "NobKLfzpxj9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/skimlit_example_abstracts.json"
      ],
      "metadata": {
        "id": "_T9Tq7Kw1BLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on more data\n",
        "lines = get_lines(\"/content/skimlit_example_abstracts.json\")"
      ],
      "metadata": {
        "id": "AMmGtUTawB96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Firs we need to split each sample\n",
        "sentence = \"\"\n",
        "samples = []\n",
        "for character in lines[0]:\n",
        "    if character == \"}\":\n",
        "      sentence += character\n",
        "      samples.append(sentence)\n",
        "      sentence = \"\"\n",
        "\n",
        "    elif character == \"{\":\n",
        "        sentence = \"\"\n",
        "    sentence += character\n",
        "\n",
        "samples[1]"
      ],
      "metadata": {
        "id": "gCBQ4o_J2nb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we need to split our data into sentences\n",
        "ex_01_sentences = []\n",
        "for sentence in samples:\n",
        "    ex_01_sentences.append(sentence.split(\".\"))\n",
        "\n",
        "len(ex_01_sentences[3])"
      ],
      "metadata": {
        "id": "qRFW3br2z8JV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the example dataset\n",
        "ex_01_dataset = []\n",
        "for i, sample in enumerate(ex_01_sentences):\n",
        "    for j, sentence in enumerate(ex_01_sentences[i]):\n",
        "        ex_01_sample = {\"sentence\": ex_01_sentences[i][j],\n",
        "                        \"line_number\": (len(ex_01_sentences[i]) - len(ex_01_sentences[i])) + j,\n",
        "                        \"total_line\": len(ex_01_sentences[i])}\n",
        "        ex_01_dataset.append(ex_01_sample)\n",
        "ex_01_dataset = pd.DataFrame(ex_01_dataset)"
      ],
      "metadata": {
        "id": "ENljeO0rlrOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Our dataset is done, but due to missing labels we're not able to predict on this dataset"
      ],
      "metadata": {
        "id": "yg1zLC_0n6p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ex_01_dataset"
      ],
      "metadata": {
        "id": "9eB9cUwMr76l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now onto the extra-curriculum"
      ],
      "metadata": {
        "id": "B9E6gCDBsXJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises:\n"
      ],
      "metadata": {
        "id": "5I9KON62tGO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model_5 on all of the data in the training dataset for as many epochs until it stops improving. Since this might take a while, you might want to use:\n",
        "\n",
        "  * tf.keras.callbacks.ModelCheckpoint to save the model's best weights only.\n",
        "  * tf.keras.callbacks.EarlyStopping to stop the model from training once the validation loss has stopped improving for ~3 epochs."
      ],
      "metadata": {
        "id": "-7_KzrgusyH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the callbacks\n",
        "model_callback_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"/content/ModelCheckpoints\", monitor=\"accuracy\", save_weights_only=True)\n",
        "model_callback_earlystopping = tf.keras.callbacks.EarlyStopping(monitor=\"accuracy\", patience=3)\n",
        "\n",
        "# Fit the model\n",
        "model_5.fit(train_char_token_pos_dataset,\n",
        "            epochs=3,\n",
        "            steps_per_epoch=int(0.10 * len(train_char_token_pos_dataset)),\n",
        "            validation_data=val_char_token_pos_dataset,\n",
        "            validation_steps=int(0.1 * len(val_char_token_pos_dataset)),\n",
        "            callbacks=[model_callback_checkpoint, model_callback_earlystopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSaUIJhEteDd",
        "outputId": "7a20e7bd-0d55-4cfb-f5c8-e0da45f21bb1"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "562/562 [==============================] - 36s 65ms/step - loss: 0.8956 - accuracy: 0.8672 - val_loss: 0.9138 - val_accuracy: 0.8441\n",
            "Epoch 2/3\n",
            "562/562 [==============================] - 36s 64ms/step - loss: 0.8999 - accuracy: 0.8624 - val_loss: 0.9083 - val_accuracy: 0.8561\n",
            "Epoch 3/3\n",
            "562/562 [==============================] - 30s 54ms/step - loss: 0.8966 - accuracy: 0.8619 - val_loss: 0.9151 - val_accuracy: 0.8484\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbcff18aaf0>"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkout the Keras guide on using pretrained GloVe embeddings. Can you get this working with one of our models?\n",
        "* Hint: You'll want to incorporate it with a custom token Embedding layer.\n",
        "* It's up to you whether or not you fine-tune the GloVe embeddings or leave them frozen."
      ],
      "metadata": {
        "id": "Q6-K6qwoy32x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaasgAzDy7Ne",
        "outputId": "487e2975-a42b-4932-eddf-bc2774b0bda7"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-13 14:39:29--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-02-13 14:39:29--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-02-13 14:39:30--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: glove.6B.zip\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.07MB/s    in 2m 40s  \n",
            "\n",
            "2023-02-13 14:42:10 (5.15 MB/s) - glove.6B.zip saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_glove_file = \"/content/glove.6B.100d.txt\"\n",
        "\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pjk2E3wKzJmB",
        "outputId": "544c1dd1-1cda-429c-906d-3e399eb020e9"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = dict(zip(rct_20k_text_vocab, range(len(rct_20k_text_vocab))))"
      ],
      "metadata": {
        "id": "AdPE8UPB5OS7"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(rct_20k_text_vocab) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR2LnRRL0Sl-",
        "outputId": "e72dc255-38df-4cb0-a282-8339504e22bc"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 29730 words (35111 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=True,\n",
        ")"
      ],
      "metadata": {
        "id": "a6R3Yi4r0VHA"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        " max_tokens= num_tokens,\n",
        " output_mode='int',\n",
        " output_sequence_length=100)\n",
        "\n",
        "vectorize_layer.adapt(train_sentences)"
      ],
      "metadata": {
        "id": "E_-JGAuIAWec"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a embedding layer for our X_of_Y dataset\n",
        "embedding_layer_X_of_Y = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    # embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=True,\n",
        ")\n",
        "\n",
        "vectorize_layer_X_of_Y = tf.keras.layers.TextVectorization(\n",
        "    max_tokens = num_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=50)\n",
        "vectorize_layer_X_of_Y.adapt(X_of_Y)"
      ],
      "metadata": {
        "id": "2GmL8QUebR31"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 1D conv model to process sequences\n",
        "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
        "text_vectors = vectorize_layer(inputs) # vectorize text inputs\n",
        "token_embeddings = embedding_layer(text_vectors) # create embedding\n",
        "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(token_embeddings)\n",
        "x = layers.GlobalAveragePooling1D()(x) # condense the output of our feature vector from conv layer\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "model_1 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile \n",
        "model_1.compile(loss=\"CategoricalCrossentropy\",\n",
        "                optimizer=\"adam\",\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg0EaxQn0lgK",
        "outputId": "8bfa473a-34e6-4ca4-f07a-40d5a376ea48"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_2 (TextV  (None, 100)              0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 100, 100)          6484300   \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 100, 64)           32064     \n",
            "                                                                 \n",
            " global_average_pooling1d_1   (None, 64)               0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,516,689\n",
            "Trainable params: 6,516,689\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "model_1.fit(train_dataset,\n",
        "            epochs=3,\n",
        "            validation_data=valid_dataset) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3CfESwR8M0n",
        "outputId": "2819b062-b782-4df8-f8f0-24a9e9e6cd06"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "5627/5627 [==============================] - 73s 13ms/step - loss: 0.6496 - accuracy: 0.7640 - val_loss: 0.5633 - val_accuracy: 0.8012\n",
            "Epoch 2/3\n",
            "5627/5627 [==============================] - 38s 7ms/step - loss: 0.4916 - accuracy: 0.8276 - val_loss: 0.5361 - val_accuracy: 0.8099\n",
            "Epoch 3/3\n",
            "5627/5627 [==============================] - 38s 7ms/step - loss: 0.4240 - accuracy: 0.8527 - val_loss: 0.5364 - val_accuracy: 0.8122\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbb81d2b4c0>"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happens if you were to merge our line_number and total_lines features for each sequence? For example, created a X_of_Y feature instead? Does this effect model performance?\n",
        "\n",
        "* Another example: line_number=1 and total_lines=11 turns into line_of_X=1_of_11.\n"
      ],
      "metadata": {
        "id": "9h0aBmHC8S5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_line_number = train_df[\"line_number\"]\n",
        "train_total_line = train_df[\"total_lines\"]"
      ],
      "metadata": {
        "id": "mzCdIyoIIMA7"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_line_number[:13])\n",
        "print(train_total_line[:13])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-0qHjhZI7t_",
        "outputId": "cbacf8eb-e2e3-47f2-f1a4-087eb783cc4a"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      0\n",
            "1      1\n",
            "2      2\n",
            "3      3\n",
            "4      4\n",
            "5      5\n",
            "6      6\n",
            "7      7\n",
            "8      8\n",
            "9      9\n",
            "10    10\n",
            "11    11\n",
            "12     0\n",
            "Name: line_number, dtype: int64\n",
            "0     11\n",
            "1     11\n",
            "2     11\n",
            "3     11\n",
            "4     11\n",
            "5     11\n",
            "6     11\n",
            "7     11\n",
            "8     11\n",
            "9     11\n",
            "10    11\n",
            "11    11\n",
            "12    10\n",
            "Name: total_lines, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the new dataset with X of Y\n",
        "X_of_Y = []\n",
        "for i, j in enumerate(train_line_number):\n",
        "    X_of_Y.append(str(train_line_number[i]) + \"_of_\" + str(train_total_line[i]))\n",
        "len(X_of_Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh-HMZ6cJK9u",
        "outputId": "b3ceda02-0bbf-410b-a7ac-e7ec871deb16"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "180040"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, one-hot-encode our data\n",
        "one_hot_encoder = OneHotEncoder()\n",
        "\n",
        "# Fitting on the training dataframe \n",
        "one_hot_encoder.fit(np.expand_dims(X_of_Y , axis = 1))\n",
        "\n",
        "# Transforming both train and val df \n",
        "train_line_number_total_encoded = one_hot_encoder.transform(np.expand_dims(X_of_Y , axis =1))\n",
        "\n",
        "# Preparing our dataset to our model\n",
        "train_line_number_total_encoded = train_line_number_total_encoded.toarray()\n",
        "train_line_number_total_encoded = tf.cast(train_line_number_total_encoded , dtype= tf.int32)"
      ],
      "metadata": {
        "id": "Ix5b-6_2fYiv"
      },
      "execution_count": 267,
      "outputs": []
    }
  ]
}